{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c4649b5",
   "metadata": {},
   "source": [
    "# Modelado - Transformador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d176b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, precision_recall_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba4a682",
   "metadata": {},
   "source": [
    "## 1. Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "488cb40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 9603\n",
    "\n",
    "K_ITERS = 10\n",
    "\n",
    "TRAIN_SIZE = 0.75\n",
    "\n",
    "caracteristicas_numericas = [\n",
    "  'flow_duration', 'total_fwd_packet', 'total_bwd_packets', \n",
    "  'total_length_of_fwd_packet', 'total_length_of_bwd_packet', \n",
    "  'fwd_packet_length_max', 'fwd_packet_length_min', 'fwd_packet_length_mean', \n",
    "  'fwd_packet_length_std', 'bwd_packet_length_max', 'bwd_packet_length_min', \n",
    "  'bwd_packet_length_mean', 'bwd_packet_length_std', 'flow_bytes/s', \n",
    "  'flow_packets/s', 'flow_iat_mean', 'flow_iat_std', 'flow_iat_max', \n",
    "  'flow_iat_min', 'fwd_iat_total', 'fwd_iat_mean', 'fwd_iat_std', \n",
    "  'fwd_iat_max', 'fwd_iat_min', 'bwd_iat_total', 'bwd_iat_mean', \n",
    "  'bwd_iat_std', 'bwd_iat_max', 'bwd_iat_min', 'fwd_psh_flags', \n",
    "  'bwd_psh_flags', 'fwd_header_length', 'bwd_header_length', 'fwd_packets/s', \n",
    "  'bwd_packets/s', 'packet_length_min', 'packet_length_max', 'packet_length_mean', \n",
    "  'packet_length_std', 'packet_length_variance', 'fin_flag_count', 'syn_flag_count',\n",
    "  'rst_flag_count', 'psh_flag_count', 'ack_flag_count', 'cwr_flag_count', \n",
    "  'ece_flag_count', 'down/up_ratio', 'average_packet_size', 'fwd_segment_size_avg', \n",
    "  'bwd_segment_size_avg', 'fwd_bytes/bulk_avg', 'fwd_packet/bulk_avg', \n",
    "  'fwd_bulk_rate_avg', 'bwd_bytes/bulk_avg', 'bwd_packet/bulk_avg', \n",
    "  'bwd_bulk_rate_avg', 'subflow_fwd_packets', 'subflow_fwd_bytes', \n",
    "  'subflow_bwd_packets', 'subflow_bwd_bytes', 'fwd_init_win_bytes', \n",
    "  'bwd_init_win_bytes', 'fwd_act_data_pkts', 'fwd_seg_size_min', \n",
    "  'active_mean', 'active_std', 'active_max', 'active_min', \n",
    "  'idle_mean', 'idle_std', 'idle_max', 'idle_min'\n",
    "]\n",
    "\n",
    "caracteristicas_var_null = ['flow_bytes/s', 'flow_iat_mean', 'flow_iat_std', 'flow_iat_max', 'flow_iat_min']\n",
    "\n",
    "caracteristicas_var_max_inf = ['flow_bytes/s', 'flow_packets/s']\n",
    "\n",
    "caracteristicas_nominales = ['port_type_registered', 'port_type_well_known', 'protocol_6', 'protocol_17']\n",
    "\n",
    "caracteristica_objetivo = \"label\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e3944b",
   "metadata": {},
   "source": [
    "## 2. Creación de transformador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "859038b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(BaseEstimator, TransformerMixin) : \n",
    "  def __init__(self, seed, categorical_chars, continous_chars, obj_char, obj_char_majority_class, selected_chars=None, imputer_strategy='median', threshold_redundance=0.9, inf_chars=[], null_chars=[], ordered_labels=[]):\n",
    "    allowed_meassures = {'mean', 'median'}\n",
    "    super().__init__()\n",
    "    self.seed = seed\n",
    "    self.categorical_chars = categorical_chars \n",
    "    self.continous_chars = continous_chars\n",
    "    self.obj_char = obj_char\n",
    "    self.obj_char_majority_class = obj_char_majority_class\n",
    "    self.selected_chars = selected_chars\n",
    "    self.threshold_redundance = threshold_redundance\n",
    "    self.inf_chars = inf_chars\n",
    "    self.null_chars = null_chars\n",
    "    self.ordered_labels = ordered_labels\n",
    "    if imputer_strategy not in allowed_meassures : \n",
    "      raise ValueError(f\"Valor inválido: {imputer_strategy}. Debe ser uno de {allowed_meassures}\")\n",
    "    self.imputer_strategy = imputer_strategy\n",
    "    \n",
    "  def inf_data_imputation(self, X_y : pd.DataFrame) : \n",
    "    X_y_ = X_y.copy()\n",
    "    X_y_.replace([np.inf], np.nan, inplace=True)\n",
    "    return X_y_\n",
    "\n",
    "  def null_train_data_imputation(self, X_y : pd.DataFrame) :\n",
    "    X_y_ = X_y.copy()\n",
    "    for char in list(set(self.inf_chars+self.null_chars)) : \n",
    "      mask = X_y_[char].isna()\n",
    "      # Se obtienen las etiquetas de la variable objetivo que tienen valores nulos\n",
    "      null_values_obj_classes = X_y_[mask][self.obj_char].unique().tolist()\n",
    "      for obj_tag in null_values_obj_classes : \n",
    "        # Obtención de medida de la columna filtrandolo por la variable objetivo\n",
    "        measure_per_obj_tag = getattr(X_y_[X_y_[self.obj_char] == obj_tag][char], self.imputer_strategy)()\n",
    "\n",
    "        # Reemplazo de valores nulos por la columna obtenida\n",
    "        X_y_.loc[mask & (X_y_[self.obj_char] == obj_tag), char] = measure_per_obj_tag\n",
    "    # for char in list(set(self.inf_chars+self.null_chars)) : \n",
    "    #   mask = X_y[char].isna()\n",
    "    #   # Se obtienen las etiquetas de la variable objetivo que tienen valores nulos\n",
    "    #   null_values_obj_classes = X_y[mask][self.obj_char].unique().tolist()\n",
    "    #   for obj_tag in null_values_obj_classes : \n",
    "    #     # Obtención de medida de la columna filtrandolo por la variable objetivo\n",
    "    #     measure_per_obj_tag = getattr(X_y[X_y[self.obj_char] == obj_tag][char], self.imputer_strategy)()\n",
    "\n",
    "    #     # Reemplazo de valores nulos por la columna obtenida\n",
    "    #     X_y.loc[mask & (X_y[self.obj_char] == obj_tag), char] = measure_per_obj_tag\n",
    "    return X_y_\n",
    "  \n",
    "  def get_measures_for_imputer(self, X_y : pd.DataFrame) : \n",
    "    measures = {}\n",
    "    for label in list(set(self.inf_chars+self.null_chars)) : \n",
    "      measures[label] = getattr(X_y[label], self.imputer_strategy)()\n",
    "    self.measures = measures\n",
    "    \n",
    "  def chars_selection(self, X_y : pd.DataFrame) : \n",
    "    def obtener_indice_grupo(corr_list, char):\n",
    "      for i, group in enumerate(corr_list):\n",
    "          if char in group :\n",
    "              return i\n",
    "      return False  # Si no se encuentra \n",
    "\n",
    "    all_chars = set(self.continous_chars)\n",
    "\n",
    "    corr_matrix = X_y[self.continous_chars].corr(method='spearman').abs()\n",
    "    np.fill_diagonal(corr_matrix.values, 0)\n",
    "\n",
    "    already_added_chars = set()\n",
    "    corr_groups = []\n",
    "\n",
    "    \"\"\" Buscar grupos de características con alta correlación \"\"\"\n",
    "    for char in self.continous_chars : \n",
    "      correlated_chars = list(corr_matrix.index[corr_matrix[char] > self.threshold_redundance])\n",
    "      correlated_chars.append(char)\n",
    "      # Eliminar duplicados\n",
    "      group = list(set(correlated_chars))\n",
    "      # Verificamos si la característica no está incluida entre las características ya agregadas\n",
    "      matches = [char for char in group if char in already_added_chars]\n",
    "      if len(group) > 1 : \n",
    "        if not matches :\n",
    "          already_added_chars.update(group)\n",
    "          corr_groups.append(group)\n",
    "        else : \n",
    "          idx = obtener_indice_grupo(corr_groups, matches[0])\n",
    "          already_added_chars.update(group)\n",
    "          detected_group = set(corr_groups[idx])\n",
    "          detected_group.update(group)\n",
    "          corr_groups[idx] = list(detected_group) \n",
    "\n",
    "    \"\"\" Fusión de grupos con alta correlación con las mismas características \"\"\"\n",
    "    merged_groups = []\n",
    "\n",
    "    for sublist in corr_groups : \n",
    "      sublist_set = set(sublist)\n",
    "      new_groups = []\n",
    "      merged_group = sublist_set.copy()\n",
    "      for group in merged_groups : \n",
    "        if sublist_set & set(group) : \n",
    "          merged_group.update(group)\n",
    "        else : \n",
    "          new_groups.append(merged_group)\n",
    "      new_groups.append(merged_group)\n",
    "      merged_groups = new_groups\n",
    "\n",
    "    correlated_chars = set().union(*merged_groups)\n",
    "    no_correlated_chars = list(all_chars-correlated_chars)\n",
    "\n",
    "    \"\"\" Selección de características por Información Mutua \"\"\"\n",
    "    mi_scores = mutual_info_classif(X_y[self.categorical_chars+self.continous_chars],X_y[self.obj_char], random_state=self.seed)\n",
    "    df_mi_scores = pd.DataFrame()\n",
    "    df_mi_scores[\"char\"] = self.categorical_chars+self.continous_chars\n",
    "    df_mi_scores[\"score\"] = mi_scores\n",
    "\n",
    "    selected_chars = []\n",
    "    for group in merged_groups : \n",
    "      mask = df_mi_scores['char'].isin(group)\n",
    "      df_mi_scores_filtered = df_mi_scores[mask].sort_values(by='score', ascending=False)\n",
    "      selected_char = df_mi_scores_filtered.iloc[0]\n",
    "      selected_chars.append(selected_char['char'])\n",
    "\n",
    "    selected_chars = selected_chars+no_correlated_chars+self.categorical_chars\n",
    "    \n",
    "    mask = df_mi_scores['char'].isin(selected_chars)\n",
    "    final_selected_chars = df_mi_scores[mask].sort_values(by='score', ascending=False).head(10)['char'].to_list()\n",
    "\n",
    "    categorical_selected_chars = [char for char in self.categorical_chars if char in final_selected_chars]\n",
    "    continous_selected_chars   = [char for char in self.continous_chars if char in final_selected_chars]\n",
    "\n",
    "    self.n_categorical_selected_chars = len(categorical_selected_chars)\n",
    "    self.selected_chars = categorical_selected_chars+continous_selected_chars\n",
    "    self.selected_indices_mi = df_mi_scores[mask].sort_values(by='score', ascending=False).head(10)\n",
    "    return X_y[self.selected_chars+[self.obj_char]]\n",
    "\n",
    "  def smotenc_resample(self, X_y : pd.DataFrame) : \n",
    "    minority_classes = set(X_y[self.obj_char].unique()) - set([self.obj_char_majority_class])\n",
    "    cant_majority_class = len(X_y[X_y[self.obj_char] == self.obj_char_majority_class])\n",
    "    strategy = {\n",
    "      self.obj_char_majority_class : cant_majority_class\n",
    "    }\n",
    "    for class_ in minority_classes : \n",
    "      calc_q = int(round(cant_majority_class/len(minority_classes), 0))\n",
    "      real_q = len(X_y[X_y[self.obj_char] == class_])\n",
    "\n",
    "      strategy[class_] = calc_q if calc_q>real_q else real_q\n",
    "\n",
    "    if self.n_categorical_selected_chars > 0 :\n",
    "      smt = SMOTENC(\n",
    "        categorical_features=list(range(self.n_categorical_selected_chars)),\n",
    "        random_state=self.seed,\n",
    "        sampling_strategy=strategy\n",
    "      )\n",
    "    else :\n",
    "      smt = SMOTE(\n",
    "        random_state=self.seed,\n",
    "        sampling_strategy=strategy\n",
    "      )\n",
    "\n",
    "    X_resample, y_resample = smt.fit_resample(\n",
    "      X_y.drop([self.obj_char], axis=1),\n",
    "      X_y[self.obj_char]\n",
    "    )\n",
    "\n",
    "    df_resample = pd.DataFrame(\n",
    "      X_resample,\n",
    "       columns=self.selected_chars\n",
    "    )\n",
    "\n",
    "    df_resample[self.obj_char] = y_resample\n",
    "\n",
    "    return df_resample\n",
    "\n",
    "  def tomek_links_subsample(self, X_y : pd.DataFrame) : \n",
    "    tmk = TomekLinks(\n",
    "      sampling_strategy=[self.obj_char_majority_class]\n",
    "    )\n",
    "\n",
    "    X_subsample, y_subsample = tmk.fit_resample(\n",
    "      X_y.drop([self.obj_char], axis=1),\n",
    "      X_y[self.obj_char]\n",
    "    )\n",
    "    \n",
    "    df_subsample = pd.DataFrame(\n",
    "      X_subsample,\n",
    "       columns=self.selected_chars\n",
    "    )\n",
    "\n",
    "    df_subsample[self.obj_char] = y_subsample\n",
    "\n",
    "    return df_subsample\n",
    "\n",
    "  def null_test_data_imputation(self, X_y : pd.DataFrame) : \n",
    "    X_y_ = X_y.copy()\n",
    "    for k, v in self.measures.items() : \n",
    "      X_y_[k] = X_y_[k].fillna(v)\n",
    "    return X_y_\n",
    "\n",
    "  def fit(self, X_y) : \n",
    "    X_y = self.inf_data_imputation(X_y=X_y)\n",
    "    X_y = self.null_train_data_imputation(X_y=X_y)\n",
    "    self.get_measures_for_imputer(X_y=X_y)\n",
    "\n",
    "    self.chars_selection(X_y=X_y)\n",
    "\n",
    "    X_y = self.smotenc_resample(X_y=X_y)\n",
    "\n",
    "    X_y = self.tomek_links_subsample(X_y=X_y)\n",
    "\n",
    "    # print('Escalamiento con StandardScaler')\n",
    "    # self.scaler = StandardScaler()\n",
    "    # self.scaler.fit(X_y[self.selected_chars])\n",
    "\n",
    "  def fit_transform(self, X : pd.DataFrame, y : pd.Series) : \n",
    "    X_y = X.copy()\n",
    "    X_y[self.obj_char] = y\n",
    "    print('Limpieza de datos')\n",
    "    X_y = self.inf_data_imputation(X_y=X_y)\n",
    "    X_y = self.null_train_data_imputation(X_y=X_y)\n",
    "    self.get_measures_for_imputer(X_y=X_y)\n",
    "    print(self.measures)\n",
    "\n",
    "    print('Selección de características')\n",
    "    X_y = self.chars_selection(X_y=X_y)\n",
    "    print(self.selected_chars)\n",
    "\n",
    "    print('Sobremuestreo con SMOTENC')\n",
    "    X_y = self.smotenc_resample(X_y=X_y)\n",
    "\n",
    "    print('Submuestreo con TomekLinks')\n",
    "    X_y = self.tomek_links_subsample(X_y=X_y)\n",
    "\n",
    "    print('Codificación de variable objetivo')\n",
    "    self.encoder = OrdinalEncoder(categories=[self.ordered_labels])\n",
    "    print('aw')\n",
    "    y_array_encoded = self.encoder.fit_transform(X_y[self.obj_char].to_frame())\n",
    "    print('aw')\n",
    "    y_encoded = pd.Series(y_array_encoded.ravel(), name=self.obj_char)\n",
    "    print(X_y[self.selected_chars].shape)\n",
    "    print(y_encoded.shape)\n",
    "    print(X_y[self.selected_chars])\n",
    "    print(y_encoded)\n",
    "    return X_y[self.selected_chars].to_numpy(), y_encoded.to_numpy()\n",
    "  \n",
    "  def transform(self, X_y : pd.DataFrame) : \n",
    "    X_y = self.inf_data_imputation(X_y=X_y)\n",
    "\n",
    "    X_y = self.null_test_data_imputation(X_y=X_y)\n",
    "\n",
    "    # X_scaled = pd.DataFrame(self.scaler.transform(X_y[self.selected_chars]), columns=self.selected_chars)\n",
    "\n",
    "    return X_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f46862b",
   "metadata": {},
   "source": [
    "## 3. K-Iteraciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce60a04",
   "metadata": {},
   "source": [
    "### 3.1. Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f06fae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bagging_resample() : Función para muestreo Bagging\n",
    "def bagging_resample(X_, y_, seed) :\n",
    "  indices = np.arange(len(X_)) \n",
    "  bootstrap_indices = resample(\n",
    "    indices,\n",
    "    replace=True,\n",
    "    stratify=y_,\n",
    "    random_state=seed,\n",
    "    n_samples=int(round(len(X_)*TRAIN_SIZE, 0))\n",
    "  )\n",
    "  # np.setdiff1d permite obtener los índices que no aparecen \n",
    "  # en los indices tras el muestreo Bootstrap\n",
    "  oob_indices = np.setdiff1d(indices, bootstrap_indices)\n",
    "\n",
    "  X_train = X_.iloc[bootstrap_indices]\n",
    "  y_train = y_.iloc[bootstrap_indices]\n",
    "  X_valid = X_.iloc[oob_indices]\n",
    "  y_valid = y_.iloc[oob_indices]\n",
    "\n",
    "  return X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fda65cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicates(X_, y_) : \n",
    "  df = pd.DataFrame(\n",
    "    columns=caracteristicas_nominales+caracteristicas_numericas,\n",
    "    data=X_\n",
    "  )\n",
    "  df[caracteristica_objetivo] = y_\n",
    "  df.drop_duplicates()\n",
    "  return df[caracteristicas_nominales+caracteristicas_numericas], df[caracteristica_objetivo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37e58bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_df() : Función para guardar dataframes\n",
    "def save_df(X_, y_, save_path) : \n",
    "  df_save = X_.copy()\n",
    "  df_save[caracteristica_objetivo] = y_\n",
    "\n",
    "  df_save.to_csv(save_path, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c918e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = pd.read_csv('DB/dataset.csv')\n",
    "\n",
    "X = df_dataset.drop([caracteristica_objetivo], axis=1)\n",
    "y = df_dataset[caracteristica_objetivo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef45e0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[793494059 498241738 377997800 912782427]\n"
     ]
    }
   ],
   "source": [
    "# Declaración de valores necesarios para K-Fold\n",
    "skf = StratifiedKFold(n_splits=K_ITERS, shuffle=True, random_state=SEED)\n",
    "\n",
    "n_models = 4\n",
    "prng = np.random.RandomState(seed=SEED)\n",
    "max_int32 = np.iinfo(np.int32).max\n",
    "seeds_per_model = prng.randint(0, max_int32, size=n_models)\n",
    "print(seeds_per_model)\n",
    "\n",
    "# Contador de iteraciones\n",
    "iter_cont = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a5a61f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_model = [\n",
    "  {\n",
    "    'path_name' : 'rf', \n",
    "    'model_name' : 'RandomForestClassifier',\n",
    "    'algorithm' : RandomForestClassifier(criterion=\"gini\", class_weight='balanced', max_depth=20, n_estimators=100, bootstrap=True, random_state=seeds_per_model[0])\n",
    "  },\n",
    "  {\n",
    "    'path_name' : 'dt', \n",
    "    'model_name' : 'DecisionTreeClassifier',\n",
    "    'algorithm' : DecisionTreeClassifier(criterion=\"gini\", class_weight='balanced', max_depth=20, random_state=seeds_per_model[1])\n",
    "  },\n",
    "  {\n",
    "    'path_name' : 'mlp',\n",
    "    'model_name' : 'MLPClassifier',\n",
    "    'algorithm' : MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=100, early_stopping=True, random_state=seeds_per_model[2])\n",
    "  },\n",
    "  {\n",
    "    'path_name' : 'knn',\n",
    "    'model_name' : 'KNeighborsClassifier',\n",
    "    'algorithm' : KNeighborsClassifier(n_neighbors=3, weights='distance', metric='manhattan')\n",
    "  },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62911fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('DB/model_evaluation//1/df_train_rf.csv')\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(df[caracteristica_objetivo])\n",
    "LABELS = encoder.classes_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b6a7c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getFrequency(): Función para obtener la distribución de frecuencias de la columna label\n",
    "def getFrequency(df : pd.DataFrame, caracteristica) :\n",
    "  frecuencia = df[caracteristica].value_counts()\n",
    "  porcentaje = df[caracteristica].value_counts(normalize=True) * 100\n",
    "\n",
    "  tabla_frecuencia = pd.DataFrame({\n",
    "    \"Frecuencia\": frecuencia,\n",
    "    \"Frecuencia(%)\": porcentaje\n",
    "  })\n",
    "  \n",
    "  tabla_frecuencia[\"Frecuencia(%)\"].round(2)\n",
    "  \n",
    "  print(tabla_frecuencia)\n",
    "  print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "097d8390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_results() : Función para automatizar la generación de la Matriz de confusión y AUC de las curvas ROC y Sensibilidad Precisión\n",
    "def get_results(y_test, y_pred, threshold) :\n",
    "  y_pred_under_threshold = y_pred >= threshold\n",
    "  y_pred_for_cm = y_pred_under_threshold.astype(int)\n",
    "  \"\"\"\n",
    "  Matriz de confusión\n",
    "              Predicted\n",
    "              0     1\n",
    "  Actual 0  [[TN,   FP],\n",
    "         1   [FN,   TP]]\n",
    "  \"\"\"\n",
    "  cm = confusion_matrix(y_test, y_pred_for_cm)\n",
    "  auc_roc = roc_auc_score(y_test, y_pred)\n",
    "  precision, recall, pr_thresholds = precision_recall_curve(y_test, y_pred)\n",
    "  auc_sp = auc(recall, precision)\n",
    "  \n",
    "  return cm, auc_roc, auc_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b0db9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot_encode() : Función para codificar la variable objetivo en razón de las clases que codificó la instancia de Machine Learning\n",
    "def one_hot_encode(y, classes) : \n",
    "  one_hot = np.zeros((len(y), len(classes)))\n",
    "  for i, label in enumerate(y) :\n",
    "    one_hot[i, int(label)] = 1\n",
    "  return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac88c7ad",
   "metadata": {},
   "source": [
    "### 3.2. Ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2646aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ITERACIÓN 1\n",
      "\n",
      "ENTRENAMIENTO\n",
      "MODELO RandomForestClassifier\n",
      "Entrenamiento de la instancia RandomForestClassifier\n",
      "Limpieza de datos\n",
      "{'flow_packets/s': 75.45746085644218, 'flow_iat_max': 60257.0, 'flow_iat_mean': 17368.85, 'flow_bytes/s': 3903.935294499232, 'flow_iat_min': 3.0, 'flow_iat_std': 17836.6592163443}\n",
      "Selección de características\n",
      "['flow_duration', 'fwd_packet_length_max', 'bwd_packet_length_max', 'fwd_header_length', 'bwd_header_length', 'packet_length_max', 'packet_length_mean', 'packet_length_std', 'packet_length_variance', 'average_packet_size']\n",
      "Sobremuestreo con SMOTENC\n",
      "Submuestreo con TomekLinks\n",
      "Codificación de variable objetivo\n",
      "aw\n",
      "aw\n",
      "(165292, 10)\n",
      "(165292,)\n",
      "        flow_duration  fwd_packet_length_max  bwd_packet_length_max  \\\n",
      "0               48908                   39.0                   87.0   \n",
      "1              169989                  375.0                 5792.0   \n",
      "2            29905106                  168.0                  268.0   \n",
      "3                  50                    0.0                    0.0   \n",
      "4                  91                    0.0                    0.0   \n",
      "...               ...                    ...                    ...   \n",
      "165287       11507592                  640.0                  976.0   \n",
      "165288       11949262                  640.0                  976.0   \n",
      "165289       12810467                  640.0                  976.0   \n",
      "165290       13424226                  640.0                  976.0   \n",
      "165291       13569617                  640.0                  976.0   \n",
      "\n",
      "        fwd_header_length  bwd_header_length  packet_length_max  \\\n",
      "0                      32                 16               87.0   \n",
      "1                     208                232             5792.0   \n",
      "2                     344                224              268.0   \n",
      "3                      24                 20                0.0   \n",
      "4                      40                 20                0.0   \n",
      "...                   ...                ...                ...   \n",
      "165287                708               1067              976.0   \n",
      "165288                712               1064              976.0   \n",
      "165289                686               1089              976.0   \n",
      "165290                737               1082              976.0   \n",
      "165291                666               1044              976.0   \n",
      "\n",
      "        packet_length_mean  packet_length_std  packet_length_variance  \\\n",
      "0                55.000000          24.787093            6.144000e+02   \n",
      "1               855.000000        1666.354750            2.776738e+06   \n",
      "2                80.307692          94.352009            8.902302e+03   \n",
      "3                 0.000000           0.000000            0.000000e+00   \n",
      "4                 0.000000           0.000000            0.000000e+00   \n",
      "...                    ...                ...                     ...   \n",
      "165287           86.677319         188.248087            3.543734e+04   \n",
      "165288           86.418182         188.201202            3.541969e+04   \n",
      "165289           86.418182         188.201202            3.541969e+04   \n",
      "165290           84.342759         186.348669            3.472856e+04   \n",
      "165291           89.857945         191.114200            3.653215e+04   \n",
      "\n",
      "        average_packet_size  \n",
      "0                 55.000000  \n",
      "1                855.000000  \n",
      "2                 80.307692  \n",
      "3                  0.000000  \n",
      "4                  0.000000  \n",
      "...                     ...  \n",
      "165287            86.677319  \n",
      "165288            86.418182  \n",
      "165289            86.418182  \n",
      "165290            84.342759  \n",
      "165291            89.857945  \n",
      "\n",
      "[165292 rows x 10 columns]\n",
      "0         0.0\n",
      "1         4.0\n",
      "2         0.0\n",
      "3         8.0\n",
      "4         8.0\n",
      "         ... \n",
      "165287    9.0\n",
      "165288    9.0\n",
      "165289    9.0\n",
      "165290    9.0\n",
      "165291    9.0\n",
      "Name: label, Length: 165292, dtype: float64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (2, 165292) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     20\u001b[39m bagging_model[iter_cont][\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m] = make_pipeline(\n\u001b[32m     21\u001b[39m     Transformer(\n\u001b[32m     22\u001b[39m       seed=seeds_per_model[i], \n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     bagging_model[i][\u001b[33m'\u001b[39m\u001b[33malgorithm\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     36\u001b[39m )\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mEntrenamiento de la instancia \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbagging_model[i][\u001b[33m'\u001b[39m\u001b[33mmodel_name\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[43mbagging_model\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mValidación de la instancia \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbagging_model[i][\u001b[33m'\u001b[39m\u001b[33mmodel_name\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     43\u001b[39m X_valid, y_valid = drop_duplicates(X_valid, y_valid)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/IDS_Thesis/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/IDS_Thesis/lib/python3.12/site-packages/sklearn/pipeline.py:654\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    647\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    648\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe `transform_input` parameter can only be set if metadata \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    649\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrouting is enabled. You can enable metadata routing using \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    650\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`sklearn.set_config(enable_metadata_routing=True)`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    651\u001b[39m     )\n\u001b[32m    653\u001b[39m routed_params = \u001b[38;5;28mself\u001b[39m._check_method_params(method=\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, props=params)\n\u001b[32m--> \u001b[39m\u001b[32m654\u001b[39m Xt = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[33m\"\u001b[39m\u001b[33mPipeline\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m._log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.steps) - \u001b[32m1\u001b[39m)):\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/IDS_Thesis/lib/python3.12/site-packages/sklearn/pipeline.py:588\u001b[39m, in \u001b[36mPipeline._fit\u001b[39m\u001b[34m(self, X, y, routed_params, raw_params)\u001b[39m\n\u001b[32m    581\u001b[39m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[32m    582\u001b[39m step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    583\u001b[39m     step_idx=step_idx,\n\u001b[32m    584\u001b[39m     step_params=routed_params[name],\n\u001b[32m    585\u001b[39m     all_params=raw_params,\n\u001b[32m    586\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m588\u001b[39m X, fitted_transformer = \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPipeline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[32m    600\u001b[39m \u001b[38;5;28mself\u001b[39m.steps[step_idx] = (name, fitted_transformer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/IDS_Thesis/lib/python3.12/site-packages/joblib/memory.py:312\u001b[39m, in \u001b[36mNotMemorizedFunc.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/IDS_Thesis/lib/python3.12/site-packages/sklearn/pipeline.py:1551\u001b[39m, in \u001b[36m_fit_transform_one\u001b[39m\u001b[34m(transformer, X, y, weight, message_clsname, message, params)\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[32m   1550\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1551\u001b[39m         res = \u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_transform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1552\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1553\u001b[39m         res = transformer.fit(X, y, **params.get(\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, {})).transform(\n\u001b[32m   1554\u001b[39m             X, **params.get(\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m   1555\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/IDS_Thesis/lib/python3.12/site-packages/sklearn/utils/_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/IDS_Thesis/lib/python3.12/site-packages/sklearn/base.py:921\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, **fit_params).transform(X)\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    920\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m.transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/IDS_Thesis/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:894\u001b[39m, in \u001b[36mStandardScaler.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    892\u001b[39m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[32m    893\u001b[39m \u001b[38;5;28mself\u001b[39m._reset()\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/IDS_Thesis/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/IDS_Thesis/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:930\u001b[39m, in \u001b[36mStandardScaler.partial_fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    898\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[32m    899\u001b[39m \n\u001b[32m    900\u001b[39m \u001b[33;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    927\u001b[39m \u001b[33;03m    Fitted scaler.\u001b[39;00m\n\u001b[32m    928\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    929\u001b[39m first_call = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mn_samples_seen_\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    938\u001b[39m n_features = X.shape[\u001b[32m1\u001b[39m]\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/IDS_Thesis/lib/python3.12/site-packages/sklearn/utils/validation.py:2944\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2942\u001b[39m         out = X, y\n\u001b[32m   2943\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2944\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2945\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2946\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/IDS_Thesis/lib/python3.12/site-packages/sklearn/utils/validation.py:1055\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1053\u001b[39m         array = xp.astype(array, dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1054\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1055\u001b[39m         array = \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1056\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[32m   1057\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1058\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.format(array)\n\u001b[32m   1059\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcomplex_warning\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/IDS_Thesis/lib/python3.12/site-packages/sklearn/utils/_array_api.py:839\u001b[39m, in \u001b[36m_asarray_with_order\u001b[39m\u001b[34m(array, dtype, order, copy, xp, device)\u001b[39m\n\u001b[32m    837\u001b[39m     array = numpy.array(array, order=order, dtype=dtype)\n\u001b[32m    838\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m839\u001b[39m     array = \u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray(array)\n",
      "\u001b[31mValueError\u001b[39m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (2, 165292) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "df_validation = pd.DataFrame(columns=[\"iter\", \"model\", \"label\", \"TP\", \"TN\", \"FP\", \"FN\", \"ms\", \"AUC_PS\"])\n",
    "df_testing    = pd.DataFrame(columns=[\"iter\", \"model\", \"label\", \"TP\", \"TN\", \"FP\", \"FN\", \"Exac\", \"Prec\", \"Sens\", \"F1\", \"ms\", \"AUC_PS\"])\n",
    "\n",
    "for temp_index, test_index in skf.split(X, y) : \n",
    "  X_temp, X_test = X.iloc[temp_index], X.iloc[test_index]\n",
    "  y_temp, y_test = y.iloc[temp_index], y.iloc[test_index]\n",
    "\n",
    "  X_test, y_test = drop_duplicates(X_test, y_test)\n",
    "\n",
    "  print(f'\\n    ITERACIÓN {iter_cont+1}\\n')\n",
    "\n",
    "  print('ENTRENAMIENTO')\n",
    "  for i in range(n_models) : \n",
    "    X_train, y_train, X_valid, y_valid = bagging_resample(\n",
    "      X_temp, y_temp, seeds_per_model[i]\n",
    "    )\n",
    "\n",
    "    print(f'MODELO {bagging_model[i]['model_name']}')\n",
    "\n",
    "    bagging_model[iter_cont]['model'] = make_pipeline(\n",
    "        Transformer(\n",
    "          seed=seeds_per_model[i], \n",
    "          categorical_chars=caracteristicas_nominales, \n",
    "          continous_chars=caracteristicas_numericas, \n",
    "          obj_char=caracteristica_objetivo, \n",
    "          obj_char_majority_class='BENIGN', \n",
    "          selected_chars=None, \n",
    "          imputer_strategy='median', \n",
    "          threshold_redundance=0.8,\n",
    "          inf_chars=caracteristicas_var_max_inf, \n",
    "          null_chars=caracteristicas_var_null,\n",
    "          ordered_labels=LABELS\n",
    "        ),\n",
    "        StandardScaler(),\n",
    "        bagging_model[i]['algorithm']\n",
    "    )\n",
    "\n",
    "    print(f'Entrenamiento de la instancia {bagging_model[i]['model_name']}')\n",
    "    bagging_model[i]['model'].fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    print(f'Validación de la instancia {bagging_model[i]['model_name']}')\n",
    "    X_valid, y_valid = drop_duplicates(X_valid, y_valid)\n",
    "    valid_start = time.perf_counter()\n",
    "    y_valid_pred = bagging_model[i]['model'].predict_proba(X_valid)\n",
    "    valid_end = time.perf_counter()\n",
    "\n",
    "    for i in range(len(LABELS)) : \n",
    "      cm, auc_roc, auc_sp = get_results(\n",
    "        y_test=y_valid[:,i],\n",
    "        y_pred=y_valid_pred[:,i],\n",
    "        threshold=0.5\n",
    "      )\n",
    "      df_validation.loc[len(df_validation)] = [\n",
    "        f\"Iteración {iter+1}\",\n",
    "        bagging_model[i][\"model_name\"], \n",
    "        LABELS[i],\n",
    "        cm[1][1], \n",
    "        cm[0][0], \n",
    "        cm[0][1], \n",
    "        cm[1][0], \n",
    "        valid_end-valid_start, \n",
    "        auc_sp\n",
    "      ]\n",
    "  \n",
    "  print('PRUEBA')\n",
    "  models_results = [\n",
    "  {\n",
    "    'path_name' : 'rf', \n",
    "    'model_name' : 'RandomForestClassifier',\n",
    "  },\n",
    "  {\n",
    "    'path_name' : 'dt', \n",
    "    'model_name' : 'DecisionTreeClassifier',\n",
    "  },\n",
    "  {\n",
    "    'path_name' : 'mlp',\n",
    "    'model_name' : 'MLPClassifier',\n",
    "  },\n",
    "  {\n",
    "    'path_name' : 'knn',\n",
    "    'model_name' : 'KNeighborsClassifier',\n",
    "  },\n",
    "  {\n",
    "    'path_name' : 'bagging',\n",
    "    'model_name' : 'IDSBaggingClassifier',\n",
    "  },\n",
    "  ]\n",
    "  y_test = one_hot_encode(\n",
    "    encoder.transform(y_test.values.ravel()), \n",
    "    LABELS\n",
    "  )\n",
    "  bagging_start = time.perf_counter()\n",
    "  y_pred_bagging = np.zeros(len(X_test), len(LABELS))\n",
    "  for i in range(n_models) : \n",
    "    model_start = time.perf_counter()\n",
    "    models_results[i]['y_pred'] = bagging_model[i]['model'].predict_proba(X_test)\n",
    "    model_end = time.perf_counter()\n",
    "    models_results[i]['time'] = (model_end - model_start)*1000\n",
    "    y_pred_bagging += models_results[i]['y_pred']\n",
    "  y_pred_bagging /= len(bagging_model)\n",
    "  models_results[-1]['y_pred'] = y_pred_bagging\n",
    "  bagging_end = time.perf_counter()\n",
    "  models_results[-1]['time'] = (bagging_end - bagging_start)*1000\n",
    "\n",
    "  for result in models_results : \n",
    "    for i in range(len(LABELS)) :\n",
    "      cm, auc_roc, auc_sp = get_results(\n",
    "        y_test=y_test[:,i],\n",
    "        y_pred=result[\"y_pred\"][:,i],\n",
    "        threshold=0.5\n",
    "      )\n",
    "      exac = (cm[1][1] + cm[0][0])/(cm[1][1]+cm[0][0]+cm[0][1]+cm[1][0])\n",
    "      prec = (cm[1][1])/(cm[1][1]+cm[0][1])\n",
    "      sens = (cm[1][1])/(cm[1][1]+cm[1][0])\n",
    "      F1sc = (sens*prec*2)/(sens+prec)\n",
    "      df_testing.loc[len(df_testing)] = [\n",
    "        f\"Iteración {iter+1}\",\n",
    "        result[\"model_name\"],\n",
    "        LABELS[i],\n",
    "        cm[1][1], \n",
    "        cm[0][0], \n",
    "        cm[0][1], \n",
    "        cm[1][0], \n",
    "        exac*100,\n",
    "        prec*100,\n",
    "        sens*100,\n",
    "        F1sc*100, \n",
    "        result[\"time\"],\n",
    "        auc_sp\n",
    "      ]\n",
    "      \"\"\"\n",
    "      Matriz de confusión\n",
    "                  Predicted\n",
    "                  0     1\n",
    "      Actual 0  [[TN,   FP],\n",
    "             1   [FN,   TP]]\n",
    "      \"\"\"\n",
    "      if LABELS[i] == 'BENIGN' : \n",
    "        exac = (cm[0][0] + cm[1][1])/(cm[0][0]+cm[1][1]+cm[1][0]+cm[0][1])\n",
    "        prec = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "        sens = (cm[0][0])/(cm[0][0]+cm[0][1])\n",
    "        F1sc = (sens*prec*2)/(sens+prec)\n",
    "        df_testing.loc[len(df_testing)] = [\n",
    "          f\"Iteración {iter+1}\",\n",
    "          result[\"model_name\"],\n",
    "          'GENERAL',\n",
    "          cm[0][0], \n",
    "          cm[1][1], \n",
    "          cm[1][0], \n",
    "          cm[0][1], \n",
    "          exac*100,\n",
    "          prec*100,\n",
    "          sens*100,\n",
    "          F1sc*100, \n",
    "          result[\"time\"],\n",
    "          auc_sp\n",
    "        ]\n",
    "\n",
    "  iter_cont += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IDS_Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
