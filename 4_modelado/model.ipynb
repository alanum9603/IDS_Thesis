{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b50bd23b",
   "metadata": {},
   "source": [
    "# Modelado - Ensamble Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4acead1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, precision_recall_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbad9be6",
   "metadata": {},
   "source": [
    "## 1. Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85d46796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[793494059 498241738 377997800 912782427]\n"
     ]
    }
   ],
   "source": [
    "SEED = 9603\n",
    "\n",
    "# Número de modelos \n",
    "N_MODELS = 4\n",
    "prng = np.random.RandomState(seed=SEED)\n",
    "max_int32 = np.iinfo(np.int32).max\n",
    "SEEDS_POR_MODELO = prng.randint(0, max_int32, size=N_MODELS)\n",
    "print(SEEDS_POR_MODELO)\n",
    "\n",
    "DB_PATH = 'DB/model_evaluation/'\n",
    "\n",
    "CARACTERISTICA_OBJETIVO = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9f9183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_model = [\n",
    "  {\n",
    "    'path_name' : 'rf', \n",
    "    'model_name' : 'RandomForestClassifier',\n",
    "    'model' : RandomForestClassifier(criterion=\"gini\", class_weight='balanced', max_depth=20, n_estimators=100, bootstrap=True, random_state=SEEDS_POR_MODELO[0])\n",
    "  },\n",
    "  {\n",
    "    'path_name' : 'dt', \n",
    "    'model_name' : 'DecisionTreeClassifier',\n",
    "    'model' : DecisionTreeClassifier(criterion=\"gini\", class_weight='balanced', max_depth=20, random_state=SEEDS_POR_MODELO[1])\n",
    "  },\n",
    "  {\n",
    "    'path_name' : 'mlp',\n",
    "    'model_name' : 'MLPClassifier',\n",
    "    'model' : MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=100, early_stopping=True, random_state=SEEDS_POR_MODELO[2])\n",
    "  },\n",
    "  {\n",
    "    'path_name' : 'knn',\n",
    "    'model_name' : 'KNeighborsClassifier',\n",
    "    'model' : KNeighborsClassifier(n_neighbors=3, weights='distance', metric='manhattan')\n",
    "  },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d10905cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{DB_PATH}/1/df_train_rf.csv')\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(df[CARACTERISTICA_OBJETIVO])\n",
    "LABELS = encoder.classes_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfc090f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getFrequency(): Función para obtener la distribución de frecuencias de la columna label\n",
    "def getFrequency(df : pd.DataFrame, caracteristica) :\n",
    "  frecuencia = df[caracteristica].value_counts()\n",
    "  porcentaje = df[caracteristica].value_counts(normalize=True) * 100\n",
    "\n",
    "  tabla_frecuencia = pd.DataFrame({\n",
    "    \"Frecuencia\": frecuencia,\n",
    "    \"Frecuencia(%)\": porcentaje\n",
    "  })\n",
    "  \n",
    "  tabla_frecuencia[\"Frecuencia(%)\"].round(2)\n",
    "  \n",
    "  print(tabla_frecuencia)\n",
    "  print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f6ddac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_results() : Función para automatizar la generación de la Matriz de confusión y AUC de las curvas ROC y Sensibilidad Precisión\n",
    "def get_results(y_test, y_pred, threshold) :\n",
    "  y_pred_under_threshold = y_pred >= threshold\n",
    "  y_pred_for_cm = y_pred_under_threshold.astype(int)\n",
    "  \"\"\"\n",
    "  Matriz de confusión\n",
    "              Predicted\n",
    "              0     1\n",
    "  Actual 0  [[TN,   FP],\n",
    "         1   [FN,   TP]]\n",
    "  \"\"\"\n",
    "  cm = confusion_matrix(y_test, y_pred_for_cm)\n",
    "  auc_roc = roc_auc_score(y_test, y_pred)\n",
    "  precision, recall, pr_thresholds = precision_recall_curve(y_test, y_pred)\n",
    "  auc_sp = auc(recall, precision)\n",
    "  \n",
    "  return cm, auc_roc, auc_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "749f436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot_encode() : Función para codificar la variable objetivo en razón de las clases que codificó la instancia de Machine Learning\n",
    "def one_hot_encode(y, classes) : \n",
    "  one_hot = np.zeros((len(y), len(classes)))\n",
    "  for i, label in enumerate(y) :\n",
    "    one_hot[i, int(label)] = 1\n",
    "  return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abcdfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Iteración 1\n",
      "  \n",
      "Entrenamiento de modelo RandomForestClassifier\n",
      "Distribución de conjunto de entrenamiento\n",
      "                  Frecuencia  Frecuencia(%)\n",
      "label                                      \n",
      "BENIGN                 77747      47.036154\n",
      "PORTSCAN               13428       8.123805\n",
      "DOS_HULK               13371       8.089321\n",
      "DDOS                    8678       5.250103\n",
      "SSH_PATATOR             8678       5.250103\n",
      "DOS_SLOWLORIS           8678       5.250103\n",
      "DOS_SLOWHTTPTEST        8678       5.250103\n",
      "DOS_GOLDENEYE           8678       5.250103\n",
      "FTP_PATATOR             8678       5.250103\n",
      "BOT                     8678       5.250103\n",
      "(165292, 11)\n",
      "Entrenamiento de modelo DecisionTreeClassifier\n",
      "Distribución de conjunto de entrenamiento\n",
      "                  Frecuencia  Frecuencia(%)\n",
      "label                                      \n",
      "BENIGN                 77754      47.038397\n",
      "PORTSCAN               13428       8.123461\n",
      "DOS_HULK               13371       8.088978\n",
      "DDOS                    8678       5.249881\n",
      "DOS_SLOWLORIS           8678       5.249881\n",
      "DOS_GOLDENEYE           8678       5.249881\n",
      "DOS_SLOWHTTPTEST        8678       5.249881\n",
      "FTP_PATATOR             8678       5.249881\n",
      "SSH_PATATOR             8678       5.249881\n",
      "BOT                     8678       5.249881\n",
      "(165299, 11)\n",
      "Entrenamiento de modelo MLPClassifier\n",
      "Distribución de conjunto de entrenamiento\n",
      "                  Frecuencia  Frecuencia(%)\n",
      "label                                      \n",
      "BENIGN                 77795      47.051530\n",
      "PORTSCAN               13428       8.121447\n",
      "DOS_HULK               13371       8.086972\n",
      "DDOS                    8678       5.248579\n",
      "DOS_GOLDENEYE           8678       5.248579\n",
      "DOS_SLOWLORIS           8678       5.248579\n",
      "BOT                     8678       5.248579\n",
      "DOS_SLOWHTTPTEST        8678       5.248579\n",
      "SSH_PATATOR             8678       5.248579\n",
      "FTP_PATATOR             8678       5.248579\n",
      "(165340, 11)\n",
      "Entrenamiento de modelo KNeighborsClassifier\n",
      "Distribución de conjunto de entrenamiento\n",
      "                  Frecuencia  Frecuencia(%)\n",
      "label                                      \n",
      "BENIGN                 77756      47.039038\n",
      "PORTSCAN               13428       8.123363\n",
      "DOS_HULK               13371       8.088880\n",
      "DDOS                    8678       5.249817\n",
      "DOS_GOLDENEYE           8678       5.249817\n",
      "DOS_SLOWLORIS           8678       5.249817\n",
      "FTP_PATATOR             8678       5.249817\n",
      "SSH_PATATOR             8678       5.249817\n",
      "BOT                     8678       5.249817\n",
      "DOS_SLOWHTTPTEST        8678       5.249817\n",
      "(165301, 11)\n",
      "Validación de modelo RandomForestClassifier\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: nan",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/IDS_Thesis/lib/python3.12/site-packages/sklearn/utils/_encode.py:235\u001b[39m, in \u001b[36m_encode\u001b[39m\u001b[34m(values, uniques, check_unknown)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_to_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/IDS_Thesis/lib/python3.12/site-packages/sklearn/utils/_encode.py:174\u001b[39m, in \u001b[36m_map_to_integer\u001b[39m\u001b[34m(values, uniques)\u001b[39m\n\u001b[32m    173\u001b[39m table = _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray([\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values], device=device(values))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/IDS_Thesis/lib/python3.12/site-packages/sklearn/utils/_encode.py:167\u001b[39m, in \u001b[36m_nandict.__missing__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nan_value\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[31mKeyError\u001b[39m: nan",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m df_valid = pd.read_csv(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDB_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/df_valid_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstance[\u001b[33m'\u001b[39m\u001b[33mpath_name\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m X_valid = df_valid.drop([CARACTERISTICA_OBJETIVO], axis=\u001b[32m1\u001b[39m).values\n\u001b[32m     28\u001b[39m y_valid = one_hot_encode(\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m   \u001b[43mencoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_valid\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCARACTERISTICA_OBJETIVO\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, \n\u001b[32m     30\u001b[39m   LABELS\n\u001b[32m     31\u001b[39m )\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDistribución de conjunto de validación\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m getFrequency(df_valid, \u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/IDS_Thesis/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:134\u001b[39m, in \u001b[36mLabelEncoder.transform\u001b[39m\u001b[34m(self, y)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) == \u001b[32m0\u001b[39m:\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray([])\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/IDS_Thesis/lib/python3.12/site-packages/sklearn/utils/_encode.py:237\u001b[39m, in \u001b[36m_encode\u001b[39m\u001b[34m(values, uniques, check_unknown)\u001b[39m\n\u001b[32m    235\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[32m    236\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check_unknown:\n",
      "\u001b[31mValueError\u001b[39m: y contains previously unseen labels: nan"
     ]
    }
   ],
   "source": [
    "df_validation = pd.DataFrame(columns=[\"iter\", \"model\", \"label\", \"TP\", \"TN\", \"FP\", \"FN\", \"ms\", \"AUC_PS\"])\n",
    "df_testing    = pd.DataFrame(columns=[\"iter\", \"model\", \"label\", \"TP\", \"TN\", \"FP\", \"FN\", \"Exac\", \"Prec\", \"Sens\", \"F1\", \"ms\", \"AUC_PS\"])\n",
    "\n",
    "for iter in range(10) : \n",
    "  print(f\"\"\"\n",
    "    Iteración {iter+1}\n",
    "  \"\"\")\n",
    "  results_list = []\n",
    "  \"\"\" Entrenamiento del modelo \"\"\"\n",
    "  for instance in bagging_model : \n",
    "    print(\"Entrenamiento de modelo\", instance['model_name'])\n",
    "    \n",
    "    df_train = pd.read_csv(f\"{DB_PATH}{iter+1}/df_train_{instance['path_name']}.csv\")\n",
    "    X_train = df_train.drop([CARACTERISTICA_OBJETIVO], axis=1).values\n",
    "    y_train = encoder.transform(df_train[CARACTERISTICA_OBJETIVO].values.ravel())\n",
    "\n",
    "    print(\"Distribución de conjunto de entrenamiento\")\n",
    "    getFrequency(df_train, 'label')\n",
    "\n",
    "    instance['model'].fit(X_train, y_train)\n",
    "\n",
    "  \"\"\" Validación del modelo \"\"\"\n",
    "  for instance in bagging_model : \n",
    "    print(\"Validación de modelo\", instance['model_name'])\n",
    "\n",
    "    df_valid = pd.read_csv(f\"{DB_PATH}{iter+1}/df_valid_{instance['path_name']}.csv\")\n",
    "    X_valid = df_valid.drop([CARACTERISTICA_OBJETIVO], axis=1).values\n",
    "    y_valid = one_hot_encode(\n",
    "      encoder.transform(df_valid[CARACTERISTICA_OBJETIVO].values.ravel()), \n",
    "      LABELS\n",
    "    )\n",
    "\n",
    "    print(\"Distribución de conjunto de validación\")\n",
    "    getFrequency(df_valid, 'label')\n",
    "\n",
    "    valid_start = time.time()\n",
    "    y_valid_pred = instance['model'].predict_proba(X_valid)\n",
    "    valid_end = time.time()\n",
    "\n",
    "    for i in range(len(LABELS)) : \n",
    "      cm, auc_roc, auc_sp = get_results(\n",
    "        y_test=y_valid[:,i],\n",
    "        y_pred=y_valid_pred[:,i],\n",
    "        threshold=0.5\n",
    "      )\n",
    "      df_validation.loc[len(df_validation)] = [\n",
    "        f\"Iteración {iter+1}\",\n",
    "        instance[\"model_name\"], \n",
    "        LABELS[i],\n",
    "        cm[1][1], \n",
    "        cm[0][0], \n",
    "        cm[0][1], \n",
    "        cm[1][0], \n",
    "        valid_end-valid_start, \n",
    "        auc_sp\n",
    "      ]\n",
    "\n",
    "  \"\"\" Prueba del modelo \"\"\"\n",
    "  for instance in bagging_model :\n",
    "    X_test = pd.read_csv(f\"{DB_PATH}{iter+1}/df_test_{instance['path_name']}.csv\").drop([CARACTERISTICA_OBJETIVO], axis=1).values\n",
    "    instance['test_data'] = X_test\n",
    "  print('Pruebas del modelo')\n",
    "\n",
    "  y_test = pd.read_csv(f\"{DB_PATH}{iter+1}/df_test_rf.csv\")[CARACTERISTICA_OBJETIVO]\n",
    "  print('Distribución del conjunto de prueba')\n",
    "  getFrequency(y_test.to_frame(), 'label')\n",
    "  y_test = one_hot_encode(\n",
    "    encoder.transform(y_test.values.ravel()), \n",
    "    LABELS\n",
    "  )\n",
    "  y_pred_bagging = np.zeros_like(y_test)\n",
    "\n",
    "  bagging_start = time.perf_counter()\n",
    "  for instance in bagging_model :\n",
    "    print(\"Prueba del modelo\", instance['model_name'])\n",
    "\n",
    "    pred_start = time.perf_counter()\n",
    "    y_pred = instance[\"model\"].predict_proba(instance[\"test_data\"])\n",
    "    pred_end   = time.perf_counter()\n",
    "\n",
    "    results_list.append({\n",
    "      \"model\" : instance[\"model_name\"],\n",
    "      \"pred\" : y_pred,\n",
    "      \"time\" : (pred_end - pred_start)*1000,\n",
    "    })\n",
    "    y_pred_bagging += y_pred\n",
    "  y_pred_bagging /= len(bagging_model)\n",
    "  bagging_end = time.perf_counter()\n",
    "\n",
    "  results_list.append({\n",
    "    'model' : 'IDSBaggingClassifier',\n",
    "    'pred' : y_pred_bagging,\n",
    "    'time' : (bagging_end-bagging_start)*1000\n",
    "  })\n",
    "  print(f'results_list : {len(results_list)}')\n",
    "  \n",
    "  for result in results_list : \n",
    "    for i in range(len(LABELS)) :\n",
    "      cm, auc_roc, auc_sp = get_results(\n",
    "        y_test=y_test[:,i],\n",
    "        y_pred=result[\"pred\"][:,i],\n",
    "        threshold=0.5\n",
    "      )\n",
    "      exac = (cm[1][1] + cm[0][0])/(cm[1][1]+cm[0][0]+cm[0][1]+cm[1][0])\n",
    "      prec = (cm[1][1])/(cm[1][1]+cm[0][1])\n",
    "      sens = (cm[1][1])/(cm[1][1]+cm[1][0])\n",
    "      F1sc = (sens*prec*2)/(sens+prec)\n",
    "      df_testing.loc[len(df_testing)] = [\n",
    "        f\"Iteración {iter+1}\",\n",
    "        result[\"model\"],\n",
    "        LABELS[i],\n",
    "        cm[1][1], \n",
    "        cm[0][0], \n",
    "        cm[0][1], \n",
    "        cm[1][0], \n",
    "        exac*100,\n",
    "        prec*100,\n",
    "        sens*100,\n",
    "        F1sc*100, \n",
    "        result[\"time\"]*1000,\n",
    "        auc_sp\n",
    "      ]\n",
    "      \"\"\"\n",
    "      Matriz de confusión\n",
    "                  Predicted\n",
    "                  0     1\n",
    "      Actual 0  [[TN,   FP],\n",
    "             1   [FN,   TP]]\n",
    "      \"\"\"\n",
    "      if LABELS[i] == 'BENIGN' : \n",
    "        exac = (cm[0][0] + cm[1][1])/(cm[0][0]+cm[1][1]+cm[1][0]+cm[0][1])\n",
    "        prec = (cm[0][0])/(cm[0][0]+cm[1][0])\n",
    "        sens = (cm[0][0])/(cm[0][0]+cm[0][1])\n",
    "        F1sc = (sens*prec*2)/(sens+prec)\n",
    "        df_testing.loc[len(df_testing)] = [\n",
    "          f\"Iteración {iter+1}\",\n",
    "          result[\"model\"],\n",
    "          'GENERAL',\n",
    "          cm[0][0], \n",
    "          cm[1][1], \n",
    "          cm[1][0], \n",
    "          cm[0][1], \n",
    "          exac*100,\n",
    "          prec*100,\n",
    "          sens*100,\n",
    "          F1sc*100, \n",
    "          result[\"time\"]*1000,\n",
    "          auc_sp\n",
    "        ]\n",
    "  iter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b079e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation.to_excel(\"../5_analisis_de_resultados/DB/validation_results.xlsx\", index=False)\n",
    "df_testing.to_excel(\"../5_analisis_de_resultados/DB/testing_results.xlsx\", index=False)\n",
    "print(\"Guardado\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IDS_Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
